<!DOCTYPE html>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta charset="utf-8">
<meta name="author" content="Jane">
<meta name="description" content="STATISTICS, ALGORITHMS, DATA STRUCTURE, DATA ENGINEERING, MACHINE LEARNING, DEEP LEARNING, ETC.">
<title> [Hadoop] Hadoop 설치 및 WordCount 예제 실습 › Signal</title>
<link rel="canonical" href="http://localhost:4000/programming/2018/02/24/Hadoop_wordCount_example/">
<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,300italic,400italic" rel="stylesheet">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link href="/basic.css" rel="stylesheet">
<link href="/highlight.css" rel="stylesheet">
<link href="/index.css" rel="stylesheet">
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Signal" />
<script src="//Signal.disqus.com/embed.js" async></script>

<header>
  <h1><a href="">Signal</a></h1>
  <nav>
    <div><a href="/">Home</a><a href="/archive/">Archive</a><a href="/about/">About</a></div>
    <div><a href="https://github.com/lovesignal"><i class="fa fa-github"></i></a><a href="mailto: hmtb.signal@gmail.com"><i class="fa fa-envelope"></i></a></div>
  </nav>
</header>
<main>
  <article>
    <header>
      <h1><a href="/programming/2018/02/24/Hadoop_wordCount_example/">[Hadoop] Hadoop 설치 및 WordCount 예제 실습</a></h1>
      <time datetime="2018-02-24T00:00:00+09:00">February 24, 2018</time>
    </header>
<h2 id="hadoop">Hadoop</h2>

<p>Hadoop = HDFS + MapReduce</p>

<h3 id="mode-3가지">Mode 3가지</h3>

<h4 id="hdfs-설치방식">HDFS 설치방식</h4>

<ul>
  <li>Stand alone (독립실행모드) : 기본 실행모드. 분산저장 안함. 코딩은 가능.</li>
  <li>Pseudo-distributed (가상분산모드) : 하나의 컴퓨터에 설치</li>
  <li>Fully distributed (완전분산모드) : 여러 대의 컴퓨터에 설치</li>
</ul>

<h3 id="순서">순서</h3>

<ol>
  <li>VirtualBox 설치</li>
  <li>Ubuntu 14.04.2 설치 // 이후에는 VirtualBox 내의 Ubuntu 에서 진행</li>
  <li>JDK 설치</li>
  <li>하둡 다운로드</li>
  <li>하둡 Stand-alone 모드 구성</li>
  <li>하둡 가상 분산 모드 구성</li>
  <li>이클립스 다운로드, 설정</li>
  <li>WordCount 예제 코딩</li>
  <li>테스트</li>
</ol>

<h3 id="ubuntu에-jdk-8-설치">Ubuntu에 JDK 8 설치</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java8-installer

</code></pre></div></div>

<h3 id="하둡-다운로드-후-압축-해제">하둡 다운로드 후 압축 해제</h3>

<p>URL : <a href="https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz">https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz</a> 에서 다운로드</p>

<p>아래 명령어로 압축해제 및 설정.</p>

<p>참고 : 앞으로 나오는 /home/유저명/ 에서 “유저명” 부분은 “여러분 자신의 유저명”으로 수정하여 입력하면 된다.</p>

<h4 id="vim-설정">vim 설정</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vi .vimrc

# 아래 내용 입력 후 저장
filetype plugin on
syntax on

set number
set paste

set ruler
set laststatus=2

$ cat .vimrc     # 확인
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd Downloads
$ cp hadoop-1.2.1.tar.gz ~       # ~ : home folder
$ cd ~
$ tar zxvf hadoop-1.2.1.tar.gz
$ sudo apt-get install vim

$ vim .bashrc # 맨 끝에 아래 3줄 추가

export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_INSTALL=/home/유저명/hadoop-1.2.1
export PATH=PATH:HADOOP_INSTALL/bin

$ source .bashrc # 변경 내용 적용
$ hadoop # 메시지가 정상 출력되면 OK
</code></pre></div></div>

<h4 id="하둡-stand-alone-모드-구성-및-예제-테스트">하둡 Stand-alone 모드 구성 및 예제 테스트</h4>

<p>아래 명령어로 테스트.</p>

<p>폴더명 중복 안되게 해야 한다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd /home/유저명/hadoop-1.2.1
$ mkdir input
$ cp README.txt input
$ hadoop jar hadoop-examples-1.2.1.jar wordcount input output
# cat output/part-r-00000 # 결과 확인

</code></pre></div></div>

<ul>
  <li>
    <p>jar : 파일을 실행하는 명령어</p>
  </li>
  <li>
    <p>hadoop-examples-1.2.1.jar 파일이 하둡 예제파일</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">$ hadoop jar hadoop-examples-1.2.1.jar wordcount input output</code> : hadoop / 파일실행 / 파일명 / 할일 /입력폴더 / 출력폴더</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 출력 결과

18/02/24 14:10:18 INFO util.NativeCodeLoader: Loaded the native-hadoop library
18/02/24 14:10:18 INFO input.FileInputFormat: Total input paths to process : 1
18/02/24 14:10:18 WARN snappy.LoadSnappy: Snappy native library not loaded
18/02/24 14:10:18 INFO mapred.JobClient: Running job: job_local1707454656_0001
18/02/24 14:10:19 INFO mapred.LocalJobRunner: Waiting for map tasks
18/02/24 14:10:19 INFO mapred.LocalJobRunner: Starting task: attempt_local1707454656_0001_m_000000_0
18/02/24 14:10:19 INFO util.ProcessTree: setsid exited with exit code 0
18/02/24 14:10:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@415eb82d
18/02/24 14:10:19 INFO mapred.MapTask: Processing split: file:/home/jane/hadoop-1.2.1/input:0+1366
18/02/24 14:10:19 INFO mapred.MapTask: io.sort.mb = 100
18/02/24 14:10:19 INFO mapred.MapTask: data buffer = 79691776/99614720
18/02/24 14:10:19 INFO mapred.MapTask: record buffer = 262144/327680
18/02/24 14:10:19 INFO mapred.MapTask: Starting flush of map output
18/02/24 14:10:19 INFO mapred.MapTask: Finished spill 0
18/02/24 14:10:19 INFO mapred.Task: Task:attempt_local1707454656_0001_m_000000_0 is done. And is in the process of commiting
18/02/24 14:10:19 INFO mapred.LocalJobRunner: 
18/02/24 14:10:19 INFO mapred.Task: Task 'attempt_local1707454656_0001_m_000000_0' done.
18/02/24 14:10:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local1707454656_0001_m_000000_0
18/02/24 14:10:19 INFO mapred.LocalJobRunner: Map task executor complete.
18/02/24 14:10:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@2f017655
18/02/24 14:10:19 INFO mapred.LocalJobRunner: 
18/02/24 14:10:19 INFO mapred.Merger: Merging 1 sorted segments
18/02/24 14:10:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1832 bytes
18/02/24 14:10:19 INFO mapred.LocalJobRunner: 
18/02/24 14:10:19 INFO mapred.Task: Task:attempt_local1707454656_0001_r_000000_0 is done. And is in the process of commiting
18/02/24 14:10:19 INFO mapred.LocalJobRunner: 
18/02/24 14:10:19 INFO mapred.Task: Task attempt_local1707454656_0001_r_000000_0 is allowed to commit now
18/02/24 14:10:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1707454656_0001_r_000000_0' to output
18/02/24 14:10:19 INFO mapred.LocalJobRunner: reduce &gt; reduce
18/02/24 14:10:19 INFO mapred.Task: Task 'attempt_local1707454656_0001_r_000000_0' done.
18/02/24 14:10:19 INFO mapred.JobClient:  map 100% reduce 100%
18/02/24 14:10:19 INFO mapred.JobClient: Job complete: job_local1707454656_0001
18/02/24 14:10:19 INFO mapred.JobClient: Counters: 20
18/02/24 14:10:19 INFO mapred.JobClient:   Map-Reduce Framework
18/02/24 14:10:19 INFO mapred.JobClient:     Spilled Records=262
18/02/24 14:10:19 INFO mapred.JobClient:     Map output materialized bytes=1836
18/02/24 14:10:19 INFO mapred.JobClient:     Reduce input records=131
18/02/24 14:10:19 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
18/02/24 14:10:19 INFO mapred.JobClient:     Map input records=31
18/02/24 14:10:19 INFO mapred.JobClient:     SPLIT_RAW_BYTES=99
18/02/24 14:10:19 INFO mapred.JobClient:     Map output bytes=2055
18/02/24 14:10:19 INFO mapred.JobClient:     Reduce shuffle bytes=0
18/02/24 14:10:19 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
18/02/24 14:10:19 INFO mapred.JobClient:     Reduce input groups=131
18/02/24 14:10:19 INFO mapred.JobClient:     Combine output records=131
18/02/24 14:10:19 INFO mapred.JobClient:     Reduce output records=131
18/02/24 14:10:19 INFO mapred.JobClient:     Map output records=179
18/02/24 14:10:19 INFO mapred.JobClient:     Combine input records=179
18/02/24 14:10:19 INFO mapred.JobClient:     CPU time spent (ms)=0
18/02/24 14:10:19 INFO mapred.JobClient:     Total committed heap usage (bytes)=357564416
18/02/24 14:10:19 INFO mapred.JobClient:   File Input Format Counters 
18/02/24 14:10:19 INFO mapred.JobClient:     Bytes Read=1366
18/02/24 14:10:19 INFO mapred.JobClient:   FileSystemCounters
18/02/24 14:10:19 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=395714
18/02/24 14:10:19 INFO mapred.JobClient:     FILE_BYTES_READ=290330
18/02/24 14:10:19 INFO mapred.JobClient:   File Output Format Counters 
18/02/24 14:10:19 INFO mapred.JobClient:     Bytes Written=1326


</code></pre></div>    </div>

    <p>​</p>
  </li>
</ul>

<h4 id="하둡-가상분산-모드-구성">하둡 가상분산 모드 구성</h4>

<p><strong>5개의 프로세스</strong> : Name node, Secondary Namenode, Data node, Job tracker, Tast tracker</p>

<p>아래 순서로 네개 파일 수정해 준다.</p>

<ul>
  <li>
    <p>hadoop-env.sh : 환경변수 설정</p>
  </li>
  <li>
    <p>core-site.xml : HDFS와 MapReduce에서 공통적으로 사용할 환경정보 설정</p>
  </li>
  <li>
    <p>hdfs-site.xml : HDFS에서 사용할 환경정보 설정</p>
  </li>
  <li>
    <p>mapred-site.xml : MapReduce에서 사용할 환경정보 설정</p>
  </li>
</ul>

<h4 id="hadoop-envsh-수정">hadoop-env.sh 수정</h4>
<p>JAVA_HOME 파라미터를 실제 JDK가 설치된 경로로 수정하는 작업</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vim /home/유저명/hadoop-1.2.1/conf/hadoop-env.sh # 맨 끝에 3줄 추가

export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=/home/유저명/hadoop-1.2.1
export HADOOP_HOME_WARN_SUPPRESS="TRUE"  # Warning: $HADOOP_HOME is deprecated.

</code></pre></div></div>

<h4 id="보안-인증-관련-명령-실행">보안 인증 관련 명령 실행</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 컴퓨터끼리 연결해서 접속할 수 있게 ssh 설정

$ sudo apt-get install openssh-server
$ sudo /etc/init.d/ssh restart # ssh 재실행
$ netstat -ntl # 0:::22 있으면 OK


# 접속할 때마다 비밀번호 묻지 않게 public key 공유(리눅스의 기능)

$ ssh-keygen -t rsa # 엔터 3번
$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ ssh localhost # 처음 한번만 yes, 두번째 접속시 부터는 안 물어봄.
$ exit # exit 로 꼭 나와야 함
</code></pre></div></div>

<h4 id="core-sitexml-하둡의-핵심-설정-파일-편집">core-site.xml (하둡의 핵심 설정 파일) 편집</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vim /home/유저명/hadoop-1.2.1/conf/core-site.xml

&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/유저명/hadoop-1.2.1/hadoop-${user.name}&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div></div>
<ul>
  <li>fs.default.name : HDFS의 기본 이름을 의미. URI 형태로 사용.</li>
  <li>hadoop.tmp.dir : 하둡에서 발생하는 임시 데이터를 저장하기 위한 공간</li>
</ul>

<h4 id="hdfs-sitexml-하둡-분산-파일-관련-설정-편집">hdfs-site.xml (하둡 분산 파일 관련 설정) 편집</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vim /home/유저명/hadoop-1.2.1/conf/hdfs-site.xml

&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.name.dir&lt;/name&gt;
        &lt;value&gt;/home/유저명/hadoop-1.2.1/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.name.edits.dir&lt;/name&gt;
        &lt;value&gt;${dfs.name.dir}&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.data.dir&lt;/name&gt;
        &lt;value&gt;/home/유저명/hadoop-1.2.1/dfs/data&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div></div>
<ul>
  <li>
    <p>dfs.replication : HDFS의 저장될 데이터의 복제본 갯수를 의미. <br />
가상분산모드 : 1 <br />
완전분산모드 : 3</p>
  </li>
  <li>dfs.http.address : 네임노드용 웹서버의 주소값. <br /> 
기본값 : 0.0.0.0:50070</li>
  <li>dfs.secondary.http.address : 보조네임노드용 웹서버 주소값. <br /> 
기본값 : 0.0.0.0:50090</li>
</ul>

<h4 id="mapred-sitexml-하둡-맵리듀스-관련-설정-편집">mapred-site.xml (하둡 맵리듀스 관련 설정) 편집</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ vim /home/유저명/hadoop-1.2.1/conf/mapred-site.xml

&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;
        &lt;value&gt;localhost:9001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.local.dir&lt;/name&gt;
        &lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.system.dir&lt;/name&gt;
        &lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre></div></div>
<ul>
  <li>mapred.job.tracker : JobTracker 데몬의 주소를 의미. 데이터 노드에서 이 주소로 맵리듀스 작업을 요청.</li>
</ul>

<h4 id="namenode-포맷--namenode-는-처음에-꼭-한번-만-포맷해야-한다">namenode 포맷 ( namenode 는 처음에 꼭 한번 만 포맷해야 한다.)</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ hadoop namenode -format
</code></pre></div></div>

<h4 id="하둡-시작-및-확인">하둡 시작 및 확인</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd /home/유저명/hadoop-1.2.1
$ bin/start-all.sh
</code></pre></div></div>

<h4 id="하둡-프로세스-확인">하둡 프로세스 확인</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ jps # 5개의 프로세스 출력되면 OK : namenode, secondarynamenode, datanode, jobtracker, tasktracker

# 브라우저에서 접속해서 확인

URL : http://localhost:50070/dfshealth.jsp   // HDFS 확인
URL : http://localhost:50030/jobtracker.jsp // MapReduce 확인
</code></pre></div></div>

<h3 id="이클립스-다운로드-설정">이클립스 다운로드, 설정</h3>

<h4 id="이클립스-다운로드">이클립스 다운로드</h4>

<p>URL : <a href="http://www.eclipse.org/downloads/">http://www.eclipse.org/downloads/</a> 로 접속</p>

<p>Eclipse IDE for Java Developers 를 다운로드</p>

<h4 id="압축해제">압축해제</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd Downloads
$ tar zxvf eclipse-java-luna-SR2-linux-gtk-x86_64.tar.gz
</code></pre></div></div>

<h4 id="이클립스를-사용하여-maven-프로젝트-생성">이클립스를 사용하여 Maven 프로젝트 생성</h4>

<h5 id="maven에-대한-이해">Maven에 대한 이해</h5>

<p>https://www.slideshare.net/sunnykwak90/ss-43767933</p>

<h5 id="이클립스-실행">이클립스 실행</h5>

<p>파일 (탐색기)를 이용해서 eclipse 폴더의 eclipse 파일을 더블 클릭해서 실행</p>

<h5 id="maven-project-생성">Maven Project 생성</h5>

<p>File -&gt; New -&gt; Project -&gt; Maven -&gt; Maven Project 선택</p>

<p>-&gt; [Next] -&gt; [Next] -&gt; [Next] -&gt;</p>

<p>Group Id : kr.co.mycompant.hd</p>

<p>Artifact Id : wcount</p>

<p>-&gt; [Finish]</p>

<h5 id="pomxml-수정해서-하둡-jar-파일-설정">pom.xml 수정해서 하둡 jar 파일 설정</h5>

<p>pom.xml 을 열어서 <dependencies> 와 </dependencies> 사이에 아래 내용을 입력</p>

<p>의미 : 하둡 1.2.1 관련된 jar 파일들을 자동으로 다운로드해서 환경 설정</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;
  &lt;version&gt;1.2.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre></div></div>

<p><img src="https://github.com/lovesignal/lovesignal.github.io/blob/master/_site/img/post/2018/Programming/Hadoop_pom_xml.png?raw=true" alt="Hadoop_pom_xml" /></p>

<h3 id="wordcount-예제-코딩">WordCount 예제 코딩</h3>

<h4 id="wordcount-클래스-생성">WordCount 클래스 생성</h4>

<p><img src="https://github.com/lovesignal/lovesignal.github.io/blob/master/_site/img/post/2018/Programming/Hadoop_wordcount_class.png?raw=true" alt="Hadoop_class" /></p>

<p>sr/main/java 밑의 kr.co.mycompany.hd.wcount 에서</p>

<p>우클릭해서 -&gt; New -&gt; Class : WordCount -&gt; [Finish]</p>

<p>아래 소스를 입력</p>

<pre><code class="language-Java">package kr.co.mycompany.hd.wcount;
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {
  public static class MyMapper
    extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
    private final static LongWritable one = new LongWritable(1);
    private Text word = new Text();
    @Override
    public void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {
      String line = value.toString();
      StringTokenizer tokenizer =
        new StringTokenizer(line, "\t\r\n\f|,.()&lt;&gt; ");
      while(tokenizer.hasMoreTokens()) {
        word.set(tokenizer.nextToken().toLowerCase());
        context.write(word, one);
      }
    }// map
  }// MyMapper
  public static class MyReducer
    extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; {
    private LongWritable sumWritable = new LongWritable();
    @Override
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,
        Context context)
        throws IOException, InterruptedException {
      long sum = 0;
      for(LongWritable val : values) {
        sum += val.get();
      }
      sumWritable.set(sum);
      context.write(key, sumWritable);
    }// reduce
  }// MyReducer
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = new Job(conf, "WordCount");

    job.setJarByClass(WordCount.class);
    job.setMapperClass(MyMapper.class);
    job.setReducerClass(MyReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);

    job.setInputFormatClass(TextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.waitForCompletion(true);
  }// main
}// end
</code></pre>

<h5 id="코드-설명">코드 설명</h5>

<p><img src="https://github.com/lovesignal/lovesignal.github.io/blob/master/_site/img/post/2018/Programming/WordcountExample.png?raw=true" alt="WordcountExample" /></p>

<pre><code class="language-Java">package kr.co.mycompany.hd.wcount;

public class WordCount {
  public static class MyMapper
    extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
    // Mapper를 상속받음 &lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;
    
    private final static LongWritable one = new LongWritable(1);
    // final로 정의(상수). 공유하기 위해 static. 1을 담음. 어떤 단어가 한 번 나올 때 1이 들어가는 용도
    
    private Text word = new Text();
    // 단어 담는 용도. 이 위치에서 생성하는 게 성능 향상에 도움이 됨. 
    // Map에 생성되면 갯수만큼 호출되므로 매번 객체생성. Map에서는 객체 생성 안함. Map 들어가기 전에 생성.
    
    @Override  
    public void map(LongWritable key, Text value, Context context)
    // Text value : 텍스트가 한 줄씩 들어옴
    // Context : 나와 하둡의 연결고리
    
        throws IOException, InterruptedException {
      String line = value.toString();
      // String으로 바꿔야 자바에서 자료 관리가 가능하므로 변경
      // toStirng : String으로 바꿔줌
      
      StringTokenizer tokenizer = new StringTokenizer(line, "\t\r\n\f|,.()&lt;&gt; ");
      // StringTokenizer&lt;String, 구분자&gt;, 구분자 맨 뒤에 공백 있음.
      // Dear Bear River
      
      while(tokenizer.hasMoreTokens()) {
        word.set(tokenizer.nextToken().toLowerCase());
        context.write(word, one);
      // hasMoreTokens() : 반복문 돌면서 'hello hadoop world'
      // -&gt; 3개로 끊어짐. 'hello', 'hadoo', 'world'
          
      // word.set : 하나씩 담음
      // nextToken : 다음 토큰 가져옴
      // toLowerCase : 대소문자 구분 안하기 위해 모두 소문자로 변경
      
      // context.write(word, one) : 단어 기록. (ex) 'hello'가 1번 나왔다 : ('hello', 1)
          // 첫번째 루프 : Deer, 1 / 두번째 루프 : Bear, 1 / 세번째 루프 : River 1
      }
    }// map
  }// MyMapper
  
  
  // Reducer
  public static class MyReducer
    extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; {
    // Reducer &lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;  
      
    private LongWritable sumWritable = new LongWritable();
    
    @Override
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,
        Context context)
    // Iterable : 반복자
        
        throws IOException, InterruptedException {
      long sum = 0;
      for(LongWritable val : values) {
        sum += val.get();
      }
      // 반복문 돌면서 LongWritable로 하나씩 가져온다.
      // 배열로 두개가 들어온다고 생각(그림의 Shuffling 부분 참고). (ex) Bear[1, 1]
      
      sumWritable.set(sum);
      // 더해 줌
        
      context.write(key, sumWritable);       
    }// reduce
  }// MyReducer
    
 
  // Main method
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    // Configuration : 환경설정
   
    Job job = new Job(conf, "WordCount");
    // Job : MapReduce의 Job

    job.setJarByClass(WordCount.class);
    job.setMapperClass(MyMapper.class);
    job.setReducerClass(MyReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);

    job.setInputFormatClass(TextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));  // 입력폴더
    FileOutputFormat.setOutputPath(job, new Path(args[1]));  // 출력폴더

    job.waitForCompletion(true);
  }
}// end
</code></pre>

<h4 id="예제를-maven-install-로-jar-파일로-패키징한다">예제를 Maven Install 로 ~.jar 파일로 패키징한다.</h4>

<p>프로젝트 우클릭 -&gt; Run As -&gt; 8 Maven install</p>

<p>target 폴더에 wcount-0.0.1-SNAPSHOT.jar 이 생성된다.</p>

<h3 id="하둡-가상-분산-모드에서-생성된-jar-을-사용하여-wordcount-실행-테스트">하둡 가상 분산 모드에서 생성된 jar 을 사용하여 WordCount 실행, 테스트</h3>

<ul>
  <li>wcount-0.0.1-SNAPSHOT.jar 파일을 /home/유저명/hadoop-1.2.1 폴더에 복사한다.</li>
  <li>hadoop fs 명령을 사용하여 테스트할 파일을 복사한다.</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ hadoop fs -put READEME.txt .
</code></pre></div></div>

<h4 id="hadoop-jar-명령을-사용하여-예제를-실행테스트한다">hadoop jar 명령을 사용하여 예제를 실행/테스트한다.</h4>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ hadoop jar wcount-0.0.1-SNAPSHOT.jar kr.co.mycompany.hd.wcount.WordCount README.txt output1
</code></pre></div></div>

<h4 id="결과가-output1-폴더에-part-r-00000-이라는-파일로-저장된다">결과가 output1 폴더에 part-r-00000 이라는 파일로 저장된다.</h4>

<p>hadoop fs -cat 명령어를 통해서 학인 할 수 있다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ hadoop fs -cat output1/part-r-00000
</code></pre></div></div>

<h4 id="wc--wordcount-명령어">wc : wordcount 명령어</h4>

<p>단어수 31,  줄 179, 크기 1366</p>

<p><img src="https://github.com/lovesignal/lovesignal.github.io/blob/master/_site/img/post/2018/Programming/linux.png?raw=true" alt="linux" /></p>

<p>[참고문헌]</p>

<p>https://blog.naver.com/sungback/220381870733</p>

<p>https://www.slideshare.net/sunnykwak90/ss-43767933</p>

<p>http://naver.me/IDMfcGl3</p>

<p>https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</p>

    
    
    <hr><div id="disqus_thread"></div>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
  </article>
</main>


<footer>
    <a href="/programming/2018/02/27/HDFS-Architecture/">« [Hadoop] HDFS Architecture</a>
    <a href="/algorithms/2018/02/21/Leetcode_7_Reverse_Integer/">[Algorithms] 7. Reverse Integer »</a>
</footer>

